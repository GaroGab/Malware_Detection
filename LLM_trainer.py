import os
import pandas as pd
from datasets import Dataset, load_from_disk
from transformers import Trainer, TrainingArguments, AutoTokenizer,\
    AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from accelerate import Accelerator, DataLoaderConfiguration

# Funzione per mappare la colonna 'family' a etichette numeriche
def label_mapping(family):
    return 1 if family.lower() == 'good' else 0

# Funzione per calcolare le metriche di valutazione
# Funzione per calcolare le metriche di valutazione
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.tensor(logits).argmax(dim=-1).numpy()  # Converti logits in numpy array
    # Assicurati che labels sia un numpy array
    if isinstance(labels, torch.Tensor):
        labels = labels.numpy()

    accuracy = accuracy_score(labels, predictions)
    precision = precision_score(labels, predictions, average='binary')
    recall = recall_score(labels, predictions, average='binary')
    f1 = f1_score(labels, predictions, average='binary')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def main():
    accelerator = Accelerator(dataloader_config=DataLoaderConfiguration(dispatch_batches=None, split_batches=False))
    #export  PYTORCH_CUDA_ALLOC_CONF = expandable_segments:True

    # Verifica se una GPU è disponibile
    device = accelerator.device
    print(f"Using device: {device}")
    print(torch.cuda.is_available())
    if torch.cuda.is_available():
        print(torch.cuda.get_device_name(0))

    # Percorsi per salvare i risultati intermedi
    good_samples_path = 'good_samples.csv'
    bad_samples_path = 'bad_samples.csv'
    train_dataset_path = 'train_dataset'
    test_dataset_path = 'test_dataset'
    good_example_path = 'good_example.txt'
    bad_example_path = 'bad_example.txt'
    model_path = "./trained_model"
    results_file = "evaluation_results.txt"

    # Selezione dei campioni
    if not (os.path.exists(good_samples_path) and os.path.exists(bad_samples_path) and os.path.exists(good_example_path) and os.path.exists(bad_example_path)):
        good_samples = pd.DataFrame()
        bad_samples = pd.DataFrame()
        good_example = None
        bad_example = None

        # Leggi il dataset CSV in blocchi
        chunksize = 100  # Modifica questa dimensione a seconda della tua RAM
        total_chunks = 0
        for chunk in pd.read_csv('urban_metadata.csv', chunksize=chunksize):
            total_chunks += 1

        print(f"Numero totale di blocchi: {total_chunks}")

        chunk_iterator = pd.read_csv('urban_metadata.csv', chunksize=chunksize)
        for chunk_idx, chunk in enumerate(tqdm(chunk_iterator, total=total_chunks, desc="Processamento dei blocchi")):
            # Filtra le righe che contengono "GhidraScript" nella colonna 'txt'
            chunk = chunk[chunk['txt'].str.contains("GhidraScript", na=False)]

            # Mappa le etichette
            chunk['label'] = chunk['family'].apply(label_mapping)

            # Seleziona un esempio buono e uno cattivo non appena li troviamo
            if good_example is None and not chunk[chunk['label'] == 1].empty:
                good_example = chunk[chunk['label'] == 1].iloc[0]
            if bad_example is None and not chunk[chunk['label'] == 0].empty:
                bad_example = chunk[chunk['label'] == 0].iloc[0]

            # Filtra i campioni "good" e "not good" esclusi gli esempi
            good_chunk = chunk[(chunk['label'] == 1) & (chunk['sha'] != good_example['sha'] if good_example is not None else True)]
            bad_chunk = chunk[(chunk['label'] == 0) & (chunk['sha'] != bad_example['sha'] if bad_example is not None else True)]

            # Seleziona fino a 1000 campioni "good"
            if len(good_samples) < 1000:
                good_samples = pd.concat([good_samples, good_chunk])
                if len(good_samples) > 1000:
                    good_samples = good_samples.sample(n=1000, random_state=42)

            # Seleziona fino a 1000 campioni "not good"
            if len(bad_samples) < 1000:
                bad_samples = pd.concat([bad_samples, bad_chunk])
                if len(bad_samples) > 1000:
                    bad_samples = bad_samples.sample(n=1000, random_state=42)

            # Stop se abbiamo già 1000 campioni per ciascuna etichetta
            if len(good_samples) >= 1000 and len(bad_samples) >= 1000 and good_example is not None and bad_example is not None:
                print(f"Selezionati 1000 campioni 'good' e 1000 campioni 'not good' a blocco {chunk_idx + 1}")
                break

        print(f"Numero di campioni 'good' selezionati: {len(good_samples)}")
        print(f"Numero di campioni 'not good' selezionati: {len(bad_samples)}")

        # Salva i campioni selezionati
        good_samples.to_csv(good_samples_path, index=False)
        bad_samples.to_csv(bad_samples_path, index=False)

        # Salva gli esempi di testo
        if good_example is not None:
            with open(good_example_path, 'w', encoding='utf-8') as f:
                f.write(good_example['txt'])
        if bad_example is not None:
            with open(bad_example_path, 'w', encoding='utf-8') as f:
                f.write(bad_example['txt'])
    else:
        good_samples = pd.read_csv(good_samples_path)
        bad_samples = pd.read_csv(bad_samples_path)
        with open(good_example_path, 'r', encoding='utf-8') as f:
            good_example = f.read()
        with open(bad_example_path, 'r', encoding='utf-8') as f:
            bad_example = f.read()

    # Divisione in train e test
    if not os.path.exists(train_dataset_path) or not os.path.exists(test_dataset_path):
        # Unisci i due sotto-dataset
        df_balanced = pd.concat([good_samples, bad_samples])

        # Dividi il dataset in train e test
        print("Divisione del dataset in train e test...")
        train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42)
        print(f"Dimensione train set: {len(train_df)}")
        print(f"Dimensione test set: {len(test_df)}")

        # Converti il dataframe pandas in un dataset Hugging Face
        train_dataset = Dataset.from_pandas(train_df)
        test_dataset = Dataset.from_pandas(test_df)

        # Salva i dataset
        train_dataset.save_to_disk(train_dataset_path)
        test_dataset.save_to_disk(test_dataset_path)
    else:
        train_dataset = load_from_disk(train_dataset_path)
        test_dataset = load_from_disk(test_dataset_path)

    # Tokenizzazione
    model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"#"Qwen/Qwen2-0.5B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)#LlamaTokenizer.from_pretrained(model_name)
    def tokenize_function(examples):
        # Converti tutto il testo in stringhe
        texts = [str(text) for text in examples['txt']]
        return tokenizer(texts, padding='max_length', truncation=True, max_length=512)

    if not os.path.exists(train_dataset_path + '_tokenized') or not os.path.exists(test_dataset_path + '_tokenized'):
        # Tokenizza i dati
        print("Tokenizzazione del dataset...")
        train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=100, num_proc=4)
        test_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=100, num_proc=4)

        # Rimuovi le colonne non necessarie
        train_dataset = train_dataset.remove_columns(["txt", "sha", "family", "__index_level_0__"])
        test_dataset = test_dataset.remove_columns(["txt", "sha", "family", "__index_level_0__"])

        # Converti le etichette in tensori di PyTorch
        train_dataset = train_dataset.with_format("torch")
        test_dataset = test_dataset.with_format("torch")

        # Salva i dataset tokenizzati
        train_dataset.save_to_disk(train_dataset_path + '_tokenized')
        test_dataset.save_to_disk(test_dataset_path + '_tokenized')
    else:
        train_dataset = load_from_disk(train_dataset_path + '_tokenized')
        test_dataset = load_from_disk(test_dataset_path + '_tokenized')

    # Carica o addestra il modello
    if os.path.exists(model_path):
        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)#LlamaForSequenceClassification.from_pretrained(model_path).to(device)
        print("Modello caricato da", model_path)
    else:
        model = AutoModelForSequenceClassification(model_name)#LlamaForSequenceClassification.from_pretrained(model_name).to(device)
        print("Modello addestrato da ", model_name)

    # Definisci gli argomenti per l'addestramento
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=1,  # Reduce batch size
        per_device_eval_batch_size=1,  # Reduce batch size
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=500,
        save_total_limit=1,
        save_steps=500,
        load_best_model_at_end=True,
        gradient_accumulation_steps=4,  # Increase to simulate larger batch size
        fp16=True,  # Mixed precision training
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        dataloader_drop_last=True,  # Drop the last incomplete batch to prevent OOM
        disable_tqdm=False,
    )

    # Cancellare la cache CUDA per liberare memoria
    torch.cuda.empty_cache()

    # Trova il checkpoint più recente, se esiste
    latest_checkpoint = None
    if os.path.isdir(training_args.output_dir):
        checkpoints = [os.path.join(training_args.output_dir, d) for d in os.listdir(training_args.output_dir) if d.startswith("checkpoint")]
        if checkpoints:
            latest_checkpoint = max(checkpoints, key=os.path.getctime)

    # Inizializza il Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,  # Aggiungi la funzione di metriche
    )

    # Addestra il modello solo se non è stato già addestrato
    if not os.path.exists(model_path):
        print("Inizio addestramento del modello...")
        trainer.train(resume_from_checkpoint=latest_checkpoint)
        print("Fine addestramento del modello.")
        # Salva il modello addestrato
        trainer.save_model(model_path)
    else:
        print("Il modello esiste già, salto l'addestramento.")

    # Valutazione del modello
    print("Valutazione del modello...")
    results = trainer.evaluate()

    # Salva i risultati della valutazione su file
    with open(results_file, 'w') as f:
        for key, value in results.items():
            f.write(f"{key}: {value}\n")

    print(f"Risultati della valutazione: {results}")

    # Funzione per fare previsioni su nuovi testi
    def predict(text):
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        outputs = model(**inputs)
        predictions = outputs.logits.argmax(dim=-1)
        return "good" if predictions.item() == 1 else "not good"

    # Esempi di inferenza
    if good_example is not None:
        print("Predizione per un esempio 'good':")
        print(f"Text: {good_example[:100]}...")  # Mostra solo i primi 100 caratteri per brevità
        print(f"Prediction: {predict(good_example)}")

    if bad_example is not None:
        print("Predizione per un esempio 'not good':")
        print(f"Text: {bad_example[:100]}...")  # Mostra solo i primi 100 caratteri per brevità
        print(f"Prediction: {predict(bad_example)}")

if __name__ == '__main__':
    main()
